{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "import sys\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from lucent.modelzoo import *\n",
    "from lucent.misc.io import show\n",
    "import lucent.optvis.objectives as objectives\n",
    "import lucent.optvis.param as param\n",
    "import lucent.optvis.render as render\n",
    "import lucent.optvis.transform as transform\n",
    "from lucent.misc.channel_reducer import ChannelReducer\n",
    "from lucent.misc.io import show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = inceptionv1(pretrained=True)\n",
    "_ = model.to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_layer(model, layer, X):\n",
    "    hook = render.ModuleHook(getattr(model, layer))\n",
    "    model(X)\n",
    "    hook.close()\n",
    "    return hook.features\n",
    "\n",
    "\n",
    "@objectives.wrap_objective()\n",
    "def dot_compare(layer, acts, batch=1):\n",
    "    def inner(T):\n",
    "        pred = T(layer)[batch]\n",
    "        return -(pred * acts).sum(dim=0, keepdims=True).mean()\n",
    "\n",
    "    return inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_activation_grid_very_naive(\n",
    "    img, model, layer=\"mixed4d\", cell_image_size=48, n_steps=1024\n",
    "):\n",
    "    # First wee need, to normalize and resize the image\n",
    "    img = torch.tensor(np.transpose(img, [2, 0, 1])).to(device)\n",
    "    normalize = (\n",
    "        transform.preprocess_inceptionv1()\n",
    "        if model._get_name() == \"InceptionV1\"\n",
    "        else transform.normalize()\n",
    "    )\n",
    "    transforms = [\n",
    "        normalize,\n",
    "        torch.nn.Upsample(size=224, mode=\"bilinear\", align_corners=True),\n",
    "    ]\n",
    "    transforms_f = transform.compose(transforms)\n",
    "    # shape: (1, 3, original height of img, original width of img)\n",
    "    img = img.unsqueeze(0)\n",
    "    # shape: (1, 3, 224, 224)\n",
    "    img = transforms_f(img)\n",
    "\n",
    "    # Here we compute the activations of the layer `layer` using `img` as input\n",
    "    # shape: (layer_channels, layer_height, layer_width), the shape depends on the layer\n",
    "    acts = get_layer(model, layer, img)[0]\n",
    "    layer_channels, layer_height, layer_width = acts.shape\n",
    "    # for each position `(y, x)` in the feature map `acts`, we optimize an image\n",
    "    # to match with the features `acts[:, y, x]`\n",
    "    # This means that the total number of cells (which is the batch size here) \n",
    "    # in the grid is layer_height*layer_width.\n",
    "    nb_cells = layer_height * layer_width\n",
    "\n",
    "    # Parametrization of the of each cell in the grid\n",
    "    param_f = lambda: param.image(\n",
    "        cell_image_size, batch=nb_cells\n",
    "    )\n",
    "    params, image_f = param_f()\n",
    "\n",
    "    obj = objectives.Objective.sum(\n",
    "        [\n",
    "            # for each position in `acts`, maximize the dot product between the activations\n",
    "            # `acts` at the position (y, x) and the features of the corresponding\n",
    "            # cell image on our 'grid'. The activations at (y, x) is a vector of size\n",
    "            # `layer_channels` (this depends on the `layer`). The features\n",
    "            # of the corresponding cell on our grid is a tensor of shape\n",
    "            # (layer_channels, cell_layer_height, cell_layer_width).\n",
    "            # Note that cell_layer_width != layer_width and cell_layer_height != layer_weight\n",
    "            # because the cell image size is smaller than the image size.\n",
    "            # With `dot_compare`, we maximize the dot product between\n",
    "            # cell_activations[y_cell, x_xcell] and acts[y,x] (both of size `layer_channels`)\n",
    "            # for each possible y_cell and x_cell, then take the average to get a single\n",
    "            # number. Check `dot_compare for more details.`\n",
    "            dot_compare(layer, acts[:, y:y+1, x:x+1], batch=x + y * layer_width)\n",
    "            for i, (x, y) in enumerate(product(range(layer_width), range(layer_height)))\n",
    "        ]\n",
    "    )\n",
    "    results = render.render_vis(\n",
    "        model,\n",
    "        obj,\n",
    "        param_f,\n",
    "        thresholds=(n_steps,),\n",
    "        progress=True,\n",
    "        fixed_image_size=cell_image_size,\n",
    "        show_image=False,\n",
    "    )\n",
    "    # shape: (layer_height*layer_width, cell_image_size, cell_image_size, 3)\n",
    "    imgs = results[-1] # last step results\n",
    "    # shape: (layer_height*layer_width, 3, cell_image_size, cell_image_size)\n",
    "    imgs = imgs.transpose((0, 3, 1, 2))\n",
    "    imgs = torch.from_numpy(imgs)\n",
    "    imgs = imgs[:, :, 2:-2, 2:-2]\n",
    "    # turn imgs into a a grid\n",
    "    grid = torchvision.utils.make_grid(imgs, nrow=int(np.sqrt(nb_cells)), padding=0)\n",
    "    grid = grid.permute(1, 2, 0)\n",
    "    grid = grid.numpy()\n",
    "    render.show(grid)\n",
    "    return imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18892/1509719539.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Ressources/panda.jpg\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrender_activation_grid_very_naive\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcell_image_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m60\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1024\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "img = np.array(Image.open(\"Ressources/panda.jpg\"), dtype=np.float32)\n",
    "_ = render_activation_grid_very_naive(img, model, cell_image_size=60, n_steps=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_activation_grid_less_naive(\n",
    "    img,\n",
    "    model,\n",
    "    layer=\"mixed4d\",\n",
    "    cell_image_size=60,\n",
    "    n_groups=6,\n",
    "    n_steps=1024,\n",
    "    batch_size=64,\n",
    "):\n",
    "    # First wee need, to normalize and resize the image\n",
    "    img = torch.tensor(np.transpose(img, [2, 0, 1])).to(device)\n",
    "    normalize = (\n",
    "        transform.preprocess_inceptionv1()\n",
    "        if model._get_name() == \"InceptionV1\"\n",
    "        else transform.normalize()\n",
    "    )\n",
    "    transforms = transform.standard_transforms.copy() + [\n",
    "        normalize,\n",
    "        torch.nn.Upsample(size=224, mode=\"bilinear\", align_corners=True),\n",
    "    ]\n",
    "    transforms_f = transform.compose(transforms)\n",
    "    # shape: (1, 3, original height of img, original width of img)\n",
    "    img = img.unsqueeze(0)\n",
    "    # shape: (1, 3, 224, 224)\n",
    "    img = transforms_f(img)\n",
    "\n",
    "    # Here we compute the activations of the layer `layer` using `img` as input\n",
    "    # shape: (layer_channels, layer_height, layer_width), the shape depends on the layer\n",
    "    acts = get_layer(model, layer, img)[0]\n",
    "    # shape: (layer_height, layer_width, layer_channels)\n",
    "    acts = acts.permute(1, 2, 0)\n",
    "    # shape: (layer_height*layer_width, layer_channels)\n",
    "    acts = acts.view(-1, acts.shape[-1])\n",
    "    acts_np = acts.cpu().numpy()\n",
    "    nb_cells = acts.shape[0]\n",
    "\n",
    "    # negative matrix factorization `NMF` is used to reduce the number\n",
    "    # of channels to n_groups. This will be used as the following.\n",
    "    # Each cell image in the grid is decomposed into a sum of\n",
    "    # (n_groups+1) images. First, each cell has its own set of parameters\n",
    "    #  this is what is called `cells_params` (see below). At the same time, we have\n",
    "    # a of group of images of size 'n_groups', which also have their own image parametrized\n",
    "    # by `groups_params`. The resulting image for a given cell in the grid\n",
    "    # is the sum of its own image (parametrized by `cells_params`)\n",
    "    # plus a weighted sum of the images of the group. Each each image from the group\n",
    "    # is weighted by `groups[cell_index, group_idx]`. Basically, this is a way of having\n",
    "    # the possibility to make cells with similar activations have a similar image, because\n",
    "    # cells with similar activations will have a similar weighting for the elements\n",
    "    # of the group.\n",
    "    if n_groups > 0:\n",
    "        reducer = ChannelReducer(n_groups, \"NMF\")\n",
    "        groups = reducer.fit_transform(acts_np)\n",
    "        groups /= groups.max(0)\n",
    "    else:\n",
    "        groups = np.zeros([])\n",
    "    # shape: (layer_height*layer_width, n_groups)\n",
    "    groups = torch.from_numpy(groups)\n",
    "\n",
    "    # Parametrization of the images of the groups (we have 'n_groups' groups)\n",
    "    groups_params, groups_image_f = param.fft_image(\n",
    "        [n_groups, 3, cell_image_size, cell_image_size]\n",
    "    )\n",
    "    # Parametrization of the images of each cell in the grid (we have 'layer_height*layer_width' cells)\n",
    "    cells_params, cells_image_f = param.fft_image(\n",
    "        [nb_cells, 3, cell_image_size, cell_image_size]\n",
    "    )\n",
    "\n",
    "    # First, we need to construct the images of the grid\n",
    "    # from the parameterizations\n",
    "\n",
    "    def image_f():\n",
    "        groups_images = groups_image_f()\n",
    "        cells_images = cells_image_f()\n",
    "        X = []\n",
    "        for i in range(nb_cells):\n",
    "            x = 0.7 * cells_images[i] + 0.5 * sum(\n",
    "                groups[i, j] * groups_images[j] for j in range(n_groups)\n",
    "            )\n",
    "            X.append(x)\n",
    "        X = torch.stack(X)\n",
    "        return X\n",
    "\n",
    "    # make sure the images are between 0 and 1\n",
    "    image_f = param.to_valid_rgb(image_f, decorrelate=True)\n",
    "\n",
    "    # After constructing the cells images, we sample randomly a mini-batch of cells\n",
    "    # from the grid. This is to prevent memory overflow, especially if the grid\n",
    "    # is large.\n",
    "    def sample(image_f, batch_size):\n",
    "        def f():\n",
    "            X = image_f()\n",
    "            inds = torch.randint(0, len(X), size=(batch_size,))\n",
    "            inputs = X[inds]\n",
    "            # HACK to store indices of the mini-batch, because we need them\n",
    "            # in objective func. Might be better ways to do that\n",
    "            sample.inds = inds\n",
    "            return inputs\n",
    "\n",
    "        return f\n",
    "\n",
    "    image_f_sampled = sample(image_f, batch_size=batch_size)\n",
    "\n",
    "    # Now, we define the objective function\n",
    "\n",
    "    def objective_func(model):\n",
    "        # shape: (batch_size, layer_channels, cell_layer_height, cell_layer_width)\n",
    "        pred = model(layer)\n",
    "        # use the sampled indices from `sample` to get the corresponding targets\n",
    "        target = acts[sample.inds].to(pred.device)\n",
    "        # shape: (batch_size, layer_channels, 1, 1)\n",
    "        target = target.view(target.shape[0], target.shape[1], 1, 1)\n",
    "        dot = (pred * target).sum(dim=1).mean()\n",
    "        return -dot\n",
    "\n",
    "    obj = objectives.Objective(objective_func)\n",
    "\n",
    "    def param_f():\n",
    "        # We optimize the parametrizations of both the groups and the cells\n",
    "        params = list(groups_params) + list(cells_params)\n",
    "        return params, image_f_sampled\n",
    "\n",
    "    results = render.render_vis(\n",
    "        model,\n",
    "        obj,\n",
    "        param_f,\n",
    "        thresholds=(n_steps,),\n",
    "        show_image=False,\n",
    "        progress=True,\n",
    "        fixed_image_size=cell_image_size,\n",
    "    )\n",
    "    # shape: (layer_height*layer_width, 3, grid_image_size, grid_image_size)\n",
    "    imgs = image_f()\n",
    "    imgs = imgs.cpu().data\n",
    "    imgs = imgs[:, :, 2:-2, 2:-2]\n",
    "    # turn imgs into a a grid\n",
    "    grid = torchvision.utils.make_grid(imgs, nrow=int(np.sqrt(nb_cells)), padding=0)\n",
    "    grid = grid.permute(1, 2, 0)\n",
    "    grid = grid.numpy()\n",
    "    render.show(grid)\n",
    "    return imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1024 [00:00<?, ?it/s]c:\\Users\\nasta\\miniconda3\\envs\\adaexam\\lib\\site-packages\\lucent\\optvis\\render.py:103: UserWarning: Some layers could not be computed because the size of the image is not big enough. It is fine, as long as the noncomputed layers are not used in the objective function(exception details: 'Given input size: (1024x1x1). Calculated output size: (1024x-5x-5). Output size is too small')\n",
      "  warnings.warn(\n",
      "100%|██████████| 1024/1024 [41:07<00:00,  2.41s/it]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute '_forward_hooks'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_24188/1603173355.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Ressources/panda.jpg\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m _ = render_activation_grid_less_naive(\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcell_image_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m60\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1024\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m )\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_24188/1261631969.py\u001b[0m in \u001b[0;36mrender_activation_grid_less_naive\u001b[1;34m(img, model, layer, cell_image_size, n_groups, n_steps, batch_size)\u001b[0m\n\u001b[0;32m    120\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage_f_sampled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 122\u001b[1;33m     results = render.render_vis(\n\u001b[0m\u001b[0;32m    123\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m         \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\nasta\\miniconda3\\envs\\adaexam\\lib\\site-packages\\lucent\\optvis\\render.py\u001b[0m in \u001b[0;36mrender_vis\u001b[1;34m(model, objective_f, param_f, optimizer, transforms, thresholds, verbose, preprocess, progress, show_image, save_image, image_name, show_inline, fixed_image_size)\u001b[0m\n\u001b[0;32m    127\u001b[0m     \u001b[1;31m# Clear hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mmodule_hook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 129\u001b[1;33m         \u001b[1;32mdel\u001b[0m \u001b[0mmodule_hook\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmodule_hook\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhook\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    130\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msave_image\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute '_forward_hooks'"
     ]
    }
   ],
   "source": [
    "img = np.array(Image.open(\"Ressources/panda.jpg\"), dtype=np.float32)\n",
    "\n",
    "_ = render_activation_grid_less_naive(\n",
    "    img, model, cell_image_size=60, n_steps=1024, batch_size=64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
      ")\n",
      "<class 'torchvision.models.resnet.ResNet'>\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ResNet' object has no attribute 'layer4:1'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_24188/4294953864.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 118\u001b[1;33m _ = render_activation_grid_resnet(\n\u001b[0m\u001b[0;32m    119\u001b[0m     \u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcell_image_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m60\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1024\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m )\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_24188/4294953864.py\u001b[0m in \u001b[0;36mrender_activation_grid_resnet\u001b[1;34m(img, model, layer, cell_image_size, n_groups, n_steps, batch_size)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;31m# Compute activations of the chosen layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m     \u001b[0macts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_resnet_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# Remove batch dim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m     \u001b[0macts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0macts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Reshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[0macts_np\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0macts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\nasta\\miniconda3\\envs\\adaexam\\lib\\site-packages\\torch\\utils\\_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    114\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_24188/4294953864.py\u001b[0m in \u001b[0;36mget_resnet_layer\u001b[1;34m(model, layer_name, X)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_resnet_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayer_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;31m# Create a hook to capture the activations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0mhook\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrender\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModuleHook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayer_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Forward pass to store activations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mhook\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Close the hook to release resources\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\nasta\\miniconda3\\envs\\adaexam\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1926\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1927\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1928\u001b[1;33m         raise AttributeError(\n\u001b[0m\u001b[0;32m   1929\u001b[0m             \u001b[1;34mf\"'{type(self).__name__}' object has no attribute '{name}'\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1930\u001b[0m         )\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'ResNet' object has no attribute 'layer4:1'"
     ]
    }
   ],
   "source": [
    "from torchvision.models import resnet50\n",
    "from lucent.modelzoo.util import get_model_layers\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = resnet50(pretrained=True)\n",
    "model.to(device).eval()\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from lucent.optvis import objectives, param, transform, render\n",
    "from PIL import Image\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define the get_layer function to get activations from any specified layer\n",
    "@torch.no_grad()\n",
    "def get_resnet_layer(model, layer_name, X):\n",
    "    # Create a hook to capture the activations\n",
    "    hook = render.ModuleHook(getattr(model, layer_name))\n",
    "    model(X)  # Forward pass to store activations\n",
    "    hook.close()  # Close the hook to release resources\n",
    "    return hook.features  # Return the activations\n",
    "\n",
    "def render_activation_grid_resnet(\n",
    "    img,\n",
    "    model,\n",
    "    layer=\"layer4_2_conv3\", # layer4_2_conv3\n",
    "    cell_image_size=60,\n",
    "    n_groups=6,\n",
    "    n_steps=1024,\n",
    "    batch_size=64,\n",
    "):\n",
    "    # Normalize & resize image for ResNet-50\n",
    "    img = torch.tensor(np.transpose(img, [2, 0, 1])).to(device)\n",
    "    normalize = transform.normalize()\n",
    "    transforms_f = transform.compose([\n",
    "        *transform.standard_transforms,\n",
    "        normalize,\n",
    "        torch.nn.Upsample(size=(224, 224), mode=\"bilinear\", align_corners=True),\n",
    "    ])\n",
    "    \n",
    "    img = img.unsqueeze(0)  # (1, 3, H, W)\n",
    "    img = transforms_f(img)  # (1, 3, 224, 224)\n",
    "\n",
    "    # Compute activations of the chosen layer\n",
    "    acts = get_resnet_layer(model, layer, img)[0]  # Remove batch dim\n",
    "    acts = acts.permute(1, 2, 0).contiguous().view(-1, acts.shape[0])  # Reshape\n",
    "    acts_np = acts.detach().cpu().numpy()\n",
    "    nb_cells = acts.shape[0]\n",
    "\n",
    "    # Reduce channels with NMF\n",
    "    if n_groups > 0:\n",
    "        reducer = ChannelReducer(n_groups, \"NMF\")\n",
    "        acts_np = np.maximum(acts_np, 0)  # Fix: Remove negative values\n",
    "        groups = reducer.fit_transform(acts_np)\n",
    "        groups /= groups.max(0)\n",
    "    else:\n",
    "        groups = np.zeros([])\n",
    "\n",
    "    \n",
    "    groups = torch.from_numpy(groups)\n",
    "\n",
    "    # Image parametrization\n",
    "    groups_params, groups_image_f = param.fft_image([n_groups, 3, cell_image_size, cell_image_size])\n",
    "    cells_params, cells_image_f = param.fft_image([nb_cells, 3, cell_image_size, cell_image_size])\n",
    "\n",
    "    def image_f():\n",
    "        groups_images = groups_image_f()\n",
    "        cells_images = cells_image_f()\n",
    "        X = [\n",
    "            0.7 * cells_images[i] + 0.5 * sum(groups[i, j] * groups_images[j] for j in range(n_groups))\n",
    "            for i in range(nb_cells)\n",
    "        ]\n",
    "        return torch.stack(X)\n",
    "\n",
    "    image_f = param.to_valid_rgb(image_f, decorrelate=True)\n",
    "\n",
    "    def sample(image_f, batch_size):\n",
    "        def f():\n",
    "            X = image_f()\n",
    "            inds = torch.randint(0, len(X), size=(batch_size,))\n",
    "            sample.inds = inds\n",
    "            return X[inds]\n",
    "        return f\n",
    "\n",
    "    image_f_sampled = sample(image_f, batch_size=batch_size)\n",
    "\n",
    "    def objective_func(model):\n",
    "        pred = get_resnet_layer(model, layer, image_f_sampled())\n",
    "        target = acts[sample.inds].to(pred.device).view(pred.shape[0], pred.shape[1], 1, 1)\n",
    "        return -(pred * target).sum(dim=1).mean()\n",
    "\n",
    "    obj = objectives.Objective(objective_func)\n",
    "\n",
    "    def param_f():\n",
    "        return list(groups_params) + list(cells_params), image_f_sampled\n",
    "\n",
    "    results = render.render_vis(\n",
    "        model,\n",
    "        obj,\n",
    "        param_f,\n",
    "        thresholds=(n_steps,),\n",
    "        show_image=False,\n",
    "        progress=True,\n",
    "        fixed_image_size=cell_image_size,\n",
    "    )\n",
    "\n",
    "    imgs = image_f().cpu().data[:, :, 2:-2, 2:-2]\n",
    "    grid = torchvision.utils.make_grid(imgs, nrow=int(np.sqrt(nb_cells)), padding=0).permute(1, 2, 0).numpy()\n",
    "    render.show(grid)\n",
    "    return imgs\n",
    "\n",
    "print(model)  # Should print the full ResNet model structure\n",
    "print(type(model))\n",
    "\n",
    "_ = render_activation_grid_resnet(\n",
    "    img, model, cell_image_size=60, n_steps=1024, batch_size=64\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adaexam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
